{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 951996,
          "sourceType": "datasetVersion",
          "datasetId": 516716
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook1f44f5481d",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "raddar_chest_xrays_indiana_university_path = kagglehub.dataset_download('raddar/chest-xrays-indiana-university')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3PgHTTToIx8",
        "outputId": "87d067b1-7304-462a-d447-87538f26b087"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/raddar/chest-xrays-indiana-university?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.2G/13.2G [10:42<00:00, 22.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:13:49.409569Z",
          "iopub.execute_input": "2025-07-13T15:13:49.409821Z",
          "iopub.status.idle": "2025-07-13T15:14:07.412675Z",
          "shell.execute_reply.started": "2025-07-13T15:13:49.409794Z",
          "shell.execute_reply": "2025-07-13T15:14:07.411597Z"
        },
        "id": "wXgSWfKwoIx9"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "till now best model"
      ],
      "metadata": {
        "id": "J9w0poIzoIx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:14:07.414564Z",
          "iopub.execute_input": "2025-07-13T15:14:07.414991Z",
          "iopub.status.idle": "2025-07-13T15:14:13.946058Z",
          "shell.execute_reply.started": "2025-07-13T15:14:07.414966Z",
          "shell.execute_reply": "2025-07-13T15:14:13.945328Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2twLR_qoIx-",
        "outputId": "fe2b3e41-c03b-48bf-d9cc-1bc3fcca5bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "### 1. Data Preparation ###\n",
        "###########################\n",
        "# Add this custom collate function before creating DataLoaders\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Handle mixed data types in batch\"\"\"\n",
        "    elem = batch[0]\n",
        "    collated = {}\n",
        "\n",
        "    for key in elem:\n",
        "        if isinstance(elem[key], torch.Tensor):\n",
        "            # Stack tensors\n",
        "            collated[key] = torch.stack([item[key] for item in batch])\n",
        "        elif isinstance(elem[key], (int, float)):\n",
        "            # Convert scalars to tensor\n",
        "            collated[key] = torch.tensor([item[key] for item in batch])\n",
        "        else:\n",
        "            # Keep as list (strings, dicts, etc)\n",
        "            collated[key] = [item[key] for item in batch]\n",
        "\n",
        "    return collated\n",
        "\n",
        "class RadiologyDataset(Dataset):\n",
        "    def __init__(self, root_dir, report_csv, projection_csv, transform=None, max_samples=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(root_dir, \"images\", \"images_normalized\")\n",
        "\n",
        "        # Load and merge metadata\n",
        "        reports_df = pd.read_csv(os.path.join(root_dir, report_csv))\n",
        "        projections_df = pd.read_csv(os.path.join(root_dir, projection_csv))\n",
        "\n",
        "        # Merge on UID\n",
        "        self.metadata = pd.merge(\n",
        "            reports_df,\n",
        "            projections_df,\n",
        "            on=\"uid\",\n",
        "            how=\"inner\"\n",
        "        )\n",
        "\n",
        "        # Filter frontal X-rays only\n",
        "        self.metadata = self.metadata[self.metadata['projection'].str.lower() == 'frontal']\n",
        "        # Drop rows where image file doesn't exist\n",
        "        self.metadata['img_path'] = self.metadata['filename'].apply(lambda f: os.path.join(self.image_dir, f))\n",
        "        self.metadata = self.metadata[self.metadata['img_path'].apply(os.path.exists)]\n",
        "\n",
        "        # THEN do sampling\n",
        "        if max_samples:\n",
        "            self.metadata = self.metadata.sample(min(max_samples, len(self.metadata)), random_state=42)\n",
        "        # Define common abnormalities\n",
        "        self.abnormalities = [\n",
        "            'cardiomegaly', 'edema', 'consolidation', 'pneumonia', 'atelectasis',\n",
        "            'pneumothorax', 'pleural effusion', 'fracture', 'nodule', 'emphysema',\n",
        "            'infiltration', 'mass', 'fibrosis', 'calcification'\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Create full report text\n",
        "        self.metadata['full_report'] = self.metadata.apply(\n",
        "            lambda row: f\"FINDINGS:\\n{row['findings']}\\n\\nIMPRESSION:\\n{row['impression']}\",\n",
        "            axis=1\n",
        "        )\n",
        "        # Precompute labels\n",
        "        self.metadata['labels'] = self.metadata.apply(\n",
        "            lambda row: [1 if abn in (str(row['findings']) + str(row['impression'])).lower() else 0\n",
        "                         for abn in self.abnormalities],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata.iloc[idx]\n",
        "        img_path = row['img_path']\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
        "        except:\n",
        "            # Use a blank image if loading fails\n",
        "            image = Image.new('L', (256, 256), color=0)\n",
        "\n",
        "        # Convert labels to tensor here\n",
        "        labels = torch.tensor(row['labels'], dtype=torch.float)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'uid': row['uid'],\n",
        "            'full_report': row['full_report'],\n",
        "            'labels': labels  # Only tensor now\n",
        "        }\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # Grayscale normalization\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "root_dir =raddar_chest_xrays_indiana_university_path  # Update with your path\n",
        "dataset = RadiologyDataset(\n",
        "    root_dir=root_dir,\n",
        "    report_csv=\"indiana_reports.csv\",\n",
        "    projection_csv=\"indiana_projections.csv\",\n",
        "    transform=transform,\n",
        "    max_samples=2000  # Reduce for Kaggle constraints\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Create data loaders with custom collate\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,\n",
        "                          collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8,\n",
        "                        collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8,\n",
        "                         collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:14:13.946829Z",
          "iopub.execute_input": "2025-07-13T15:14:13.947187Z",
          "iopub.status.idle": "2025-07-13T15:14:17.992255Z",
          "shell.execute_reply.started": "2025-07-13T15:14:13.947133Z",
          "shell.execute_reply": "2025-07-13T15:14:17.991471Z"
        },
        "id": "t0UJDn0UoIx-"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "### 2. Weakly-Supervised Detection Model ###\n",
        "##############################################\n",
        "\n",
        "class CAMModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Use a ResNet model modified for grayscale\n",
        "        base_model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "        # Modify first layer for grayscale (1 channel)\n",
        "        base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Feature extractor\n",
        "        self.features = nn.Sequential(\n",
        "            base_model.conv1,\n",
        "            base_model.bn1,\n",
        "            base_model.relu,\n",
        "            base_model.maxpool,\n",
        "            base_model.layer1,\n",
        "            base_model.layer2,\n",
        "            base_model.layer3,\n",
        "            base_model.layer4\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Classifier\n",
        "\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Hook for feature maps\n",
        "        self.feature_maps = None\n",
        "        self.features.register_forward_hook(self.get_activation())\n",
        "\n",
        "    def get_activation(self):\n",
        "        def hook(module, input, output):\n",
        "            self.feature_maps = output\n",
        "        return hook\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        pooled = self.gap(features).flatten(1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "    def get_cam(self, class_idx):\n",
        "        weights = self.classifier.weight[class_idx]\n",
        "        cam = torch.einsum('nchw,c->nhw', self.feature_maps, weights)\n",
        "        return torch.relu(cam)\n",
        "\n",
        "# Initialize model\n",
        "model = CAMModel(len(dataset.abnormalities)).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training function\n",
        "def train_detector(model, train_loader, val_loader, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch['image'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
        "              f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "# Train the detector (reduce epochs for Kaggle)\n",
        "train_detector(model, train_loader, val_loader, epochs=5)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:14:17.993287Z",
          "iopub.execute_input": "2025-07-13T15:14:17.993507Z",
          "iopub.status.idle": "2025-07-13T15:22:02.181214Z",
          "shell.execute_reply.started": "2025-07-13T15:14:17.99349Z",
          "shell.execute_reply": "2025-07-13T15:22:02.180483Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udfrpb_qoIx_",
        "outputId": "3b5e777c-3d46-486f-b2ef-d2285d09bb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 144MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.3525 | Val Loss: 0.2922\n",
            "Epoch 2/5 | Train Loss: 0.2937 | Val Loss: 0.2980\n",
            "Epoch 3/5 | Train Loss: 0.2888 | Val Loss: 0.2924\n",
            "Epoch 4/5 | Train Loss: 0.2827 | Val Loss: 0.3086\n",
            "Epoch 5/5 | Train Loss: 0.2785 | Val Loss: 0.2900\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "### 3. Generate Pseudo-Bounding Boxes (CAM) ###\n",
        "################################################\n",
        "\n",
        "def cam_to_bbox(cam, threshold=0.7, min_area=0.01):\n",
        "    \"\"\"Convert CAM to bounding boxes with filtering\"\"\"\n",
        "    cam = cam.detach().cpu().numpy()\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "    mask = (cam > threshold).astype(np.uint8)\n",
        "\n",
        "    from skimage.measure import label, regionprops\n",
        "    labeled = label(mask)\n",
        "    bboxes = []\n",
        "\n",
        "    for region in regionprops(labeled):\n",
        "        y1, x1, y2, x2 = region.bbox\n",
        "        area = (x2 - x1) * (y2 - y1) / (cam.shape[1] * cam.shape[0])\n",
        "\n",
        "        # Filter small regions\n",
        "        if area < min_area:\n",
        "            continue\n",
        "\n",
        "        bboxes.append({\n",
        "            'xmin': x1 / cam.shape[1],\n",
        "            'ymin': y1 / cam.shape[0],\n",
        "            'xmax': x2 / cam.shape[1],\n",
        "            'ymax': y2 / cam.shape[0],\n",
        "            'confidence': cam[region.slice].max(),\n",
        "            'area': area\n",
        "        })\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def generate_bboxes_for_dataset(model, dataset):\n",
        "    \"\"\"Add bounding boxes to dataset items\"\"\"\n",
        "    model.eval()\n",
        "    new_data = []\n",
        "\n",
        "    for item in dataset:\n",
        "        image = item['image'].unsqueeze(0).to(device)\n",
        "        labels = item['labels'].cpu().numpy()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _ = model(image)\n",
        "\n",
        "        bboxes = []\n",
        "        for class_idx, label in enumerate(labels):\n",
        "            if label == 1:  # Only for present abnormalities\n",
        "                cam = model.get_cam(class_idx)\n",
        "                class_bboxes = cam_to_bbox(cam[0])\n",
        "                for bbox in class_bboxes:\n",
        "                    bbox['class'] = dataset.dataset.abnormalities[class_idx]\n",
        "                    bboxes.append(bbox)\n",
        "\n",
        "        # If no findings, create full-image box\n",
        "        if not bboxes and \"no finding\" in item['full_report'].lower():\n",
        "            bboxes = [{\n",
        "                'class': 'No finding',\n",
        "                'xmin': 0,\n",
        "                'ymin': 0,\n",
        "                'xmax': 1,\n",
        "                'ymax': 1,\n",
        "                'confidence': 1.0,\n",
        "                'area': 1.0\n",
        "            }]\n",
        "\n",
        "        # Sort by confidence and keep top 3\n",
        "        bboxes = sorted(bboxes, key=lambda x: x['confidence'], reverse=True)[:3]\n",
        "        new_item = item.copy()\n",
        "        new_item['bboxes'] = bboxes\n",
        "        new_data.append(new_item)\n",
        "\n",
        "    return new_data\n",
        "\n",
        "# Add bboxes to datasets\n",
        "train_dataset_with_bbox = generate_bboxes_for_dataset(model, train_dataset)\n",
        "val_dataset_with_bbox = generate_bboxes_for_dataset(model, val_dataset)\n",
        "test_dataset_with_bbox = generate_bboxes_for_dataset(model, test_dataset)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:22:02.182061Z",
          "iopub.execute_input": "2025-07-13T15:22:02.182357Z",
          "iopub.status.idle": "2025-07-13T15:23:43.865517Z",
          "shell.execute_reply.started": "2025-07-13T15:22:02.182332Z",
          "shell.execute_reply": "2025-07-13T15:23:43.864872Z"
        },
        "id": "1IFiEOWgoIyA"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "_kQVaeH6uUlV",
        "outputId": "b952a6f6-08a1-489c-939d-fc6506ae5d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "### 4. Multi-View Image Creation ###\n",
        "####################################\n",
        "\n",
        "def create_multiview(item):\n",
        "    \"\"\"Generate global, saliency, and region views\"\"\"\n",
        "    # Convert tensor to PIL image\n",
        "    image_tensor = item['image']\n",
        "    image_pil = transforms.ToPILImage()(image_tensor).convert(\"RGB\")\n",
        "    orig_size = image_pil.size\n",
        "\n",
        "    # Global view\n",
        "    global_view = image_pil.resize((224, 224))\n",
        "\n",
        "    # Create heatmap overlay\n",
        "    heatmap = Image.new('L', orig_size, 0)\n",
        "    draw = ImageDraw.Draw(heatmap)\n",
        "\n",
        "    for bbox in item['bboxes']:\n",
        "        x1 = int(bbox['xmin'] * orig_size[0])\n",
        "        y1 = int(bbox['ymin'] * orig_size[1])\n",
        "        x2 = int(bbox['xmax'] * orig_size[0])\n",
        "        y2 = int(bbox['ymax'] * orig_size[1])\n",
        "        draw.rectangle([x1, y1, x2, y2], fill=int(255 * bbox['confidence']))\n",
        "\n",
        "    # Blend with original image\n",
        "    saliency_view = Image.blend(\n",
        "        image_pil.convert(\"RGBA\"),\n",
        "        heatmap.convert(\"RGBA\"),\n",
        "        alpha=0.4\n",
        "    ).convert(\"RGB\").resize((224, 224))\n",
        "\n",
        "    # Region crops\n",
        "    region_views = []\n",
        "    for bbox in item['bboxes']:\n",
        "        x1 = int(bbox['xmin'] * orig_size[0])\n",
        "        y1 = int(bbox['ymin'] * orig_size[1])\n",
        "        x2 = int(bbox['xmax'] * orig_size[0])\n",
        "        y2 = int(bbox['ymax'] * orig_size[1])\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "\n",
        "        # Expand crop by 20% for context\n",
        "        expand_x, expand_y = int(w * 0.2), int(h * 0.2)\n",
        "        x1 = max(0, x1 - expand_x)\n",
        "        y1 = max(0, y1 - expand_y)\n",
        "        x2 = min(orig_size[0], x2 + expand_x)\n",
        "        y2 = min(orig_size[1], y2 + expand_y)\n",
        "\n",
        "        region = image_pil.crop((x1, y1, x2, y2)).resize((112, 112))\n",
        "        region_views.append(region)\n",
        "\n",
        "    # Pad to 3 regions\n",
        "    while len(region_views) < 3:\n",
        "        region_views.append(Image.new('RGB', (112, 112), (0, 0, 0)))\n",
        "\n",
        "    # Convert to tensors\n",
        "    transform_to_tensor = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        'global_view': transform_to_tensor(global_view),\n",
        "        'saliency_view': transform_to_tensor(saliency_view),\n",
        "        'region1': transform_to_tensor(region_views[0]),\n",
        "        'region2': transform_to_tensor(region_views[1]),\n",
        "        'region3': transform_to_tensor(region_views[2]),\n",
        "        'full_report': item['full_report'],\n",
        "        'bboxes': item['bboxes']\n",
        "    }\n",
        "\n",
        "# Create multiview datasets\n",
        "train_multiview = [create_multiview(item) for item in train_dataset_with_bbox]\n",
        "val_multiview = [create_multiview(item) for item in val_dataset_with_bbox]\n",
        "test_multiview = [create_multiview(item) for item in test_dataset_with_bbox]\n",
        "\n",
        "# Initialize model with reduced parameters\n",
        "VOCAB_SIZE = 5000\n",
        "\n",
        "# Build vocabulary from training reports\n",
        "word_counts = {}\n",
        "for item in train_multiview:\n",
        "    report = item['full_report'].lower()\n",
        "    words = word_tokenize(report)\n",
        "    for word in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "# Create vocabulary with special tokens\n",
        "vocab = {\n",
        "    '<PAD>': 0,\n",
        "    '<SOS>': 1,\n",
        "    '<EOS>': 2,\n",
        "    '<UNK>': 3\n",
        "}\n",
        "# Add most common words\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, count in sorted_words:\n",
        "    if len(vocab) < VOCAB_SIZE:  # Only keep top VOCAB_SIZE words\n",
        "        vocab[word] = len(vocab)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Inverse vocabulary for decoding\n",
        "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Text encoding function\n",
        "def encode_caption(report, vocab, max_length=100):\n",
        "    \"\"\"Convert report text to sequence of token indices\"\"\"\n",
        "    words = word_tokenize(report.lower())\n",
        "    tokens = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
        "    tokens = [vocab['<SOS>']] + tokens + [vocab['<EOS>']]\n",
        "\n",
        "    # Pad or truncate to max_length\n",
        "    if len(tokens) < max_length:\n",
        "        tokens += [vocab['<PAD>']] * (max_length - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_length-1] + [vocab['<EOS>']]\n",
        "\n",
        "    return torch.tensor(tokens, dtype=torch.long)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:23:43.866329Z",
          "iopub.execute_input": "2025-07-13T15:23:43.866755Z",
          "iopub.status.idle": "2025-07-13T15:23:57.823041Z",
          "shell.execute_reply.started": "2025-07-13T15:23:43.866733Z",
          "shell.execute_reply": "2025-07-13T15:23:57.822499Z"
        },
        "id": "Mre5qRG3oIyA"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "### 5. Multimodal Report Generation Model ###\n",
        "##############################################\n",
        "\n",
        "class MultimodalReportGenerator(nn.Module):\n",
        "    def __init__(self, report_vocab_size, embedding_dim=256, hidden_dim=256):  # Reduced hidden_dim\n",
        "        super().__init__()\n",
        "        # Image encoder - shared weights\n",
        "        base = torchvision.models.resnet18(pretrained=True)\n",
        "        self.encoder = nn.Sequential(*list(base.children())[:-2])\n",
        "\n",
        "        # Feature fusion\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(512 * 5, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.embedding = nn.Embedding(report_vocab_size, embedding_dim)\n",
        "\n",
        "        # Report decoder (LSTM)\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=embedding_dim + 512,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, report_vocab_size)\n",
        "\n",
        "    def encode_view(self, view):\n",
        "        \"\"\"Encode a single view and apply adaptive pooling\"\"\"\n",
        "        features = self.encoder(view)\n",
        "        return nn.AdaptiveAvgPool2d(1)(features).squeeze(-1).squeeze(-1)\n",
        "\n",
        "    def forward(self, global_view, saliency_view, regions, captions=None):\n",
        "        # Encode images\n",
        "        global_feat = self.encode_view(global_view)\n",
        "        saliency_feat = self.encode_view(saliency_view)\n",
        "        region_feats = [self.encode_view(region) for region in regions]\n",
        "\n",
        "        # Concatenate features\n",
        "        features = torch.cat([\n",
        "            global_feat,\n",
        "            saliency_feat,\n",
        "            *region_feats\n",
        "        ], dim=1)\n",
        "        fused_feat = self.feature_fusion(features)\n",
        "\n",
        "        # Initialize decoder\n",
        "        batch_size = global_view.size(0)\n",
        "        hidden = (\n",
        "            torch.zeros(1, batch_size, self.decoder.hidden_size).to(device),\n",
        "            torch.zeros(1, batch_size, self.decoder.hidden_size).to(device)\n",
        "        )\n",
        "\n",
        "        # Teacher forcing or inference\n",
        "        seq_length = 100 if captions is None else captions.size(1)\n",
        "        outputs = torch.zeros(batch_size, seq_length, self.fc.out_features).to(device)\n",
        "\n",
        "        # Start token\n",
        "        input_token = torch.zeros(batch_size, 1).long().to(device)  # <SOS>\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            # Embed input token\n",
        "            embedded = self.embedding(input_token)\n",
        "\n",
        "            # Combine with image features\n",
        "            combined = torch.cat([embedded, fused_feat.unsqueeze(1)], dim=2)\n",
        "\n",
        "            # LSTM step\n",
        "            output, hidden = self.decoder(combined, hidden)\n",
        "            output = self.fc(output.squeeze(1))\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            # Next input (teacher forcing or prediction)\n",
        "            if captions is not None and torch.rand(1).item() > 0.5:\n",
        "                input_token = captions[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                input_token = output.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, global_view, saliency_view, regions, max_length=100):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode images\n",
        "            global_feat = self.encode_view(global_view)\n",
        "            saliency_feat = self.encode_view(saliency_view)\n",
        "            region_feats = [self.encode_view(region) for region in regions]\n",
        "\n",
        "            # Concatenate features\n",
        "            features = torch.cat([\n",
        "                global_feat,\n",
        "                saliency_feat,\n",
        "                *region_feats\n",
        "            ], dim=1)\n",
        "            fused_feat = self.feature_fusion(features)\n",
        "\n",
        "            # Initialize decoder\n",
        "            batch_size = global_view.size(0)\n",
        "            hidden = (\n",
        "                torch.zeros(1, batch_size, self.decoder.hidden_size).to(device),\n",
        "                torch.zeros(1, batch_size, self.decoder.hidden_size).to(device)\n",
        "            )\n",
        "\n",
        "            # Start token\n",
        "            input_token = torch.full((batch_size, 1), vocab['<SOS>'], dtype=torch.long).to(device)\n",
        "            generated = torch.zeros(batch_size, max_length, dtype=torch.long).to(device)\n",
        "            finished = torch.zeros(batch_size, dtype=torch.bool).to(device)\n",
        "\n",
        "            for t in range(max_length):\n",
        "                embedded = self.embedding(input_token)\n",
        "                combined = torch.cat([embedded, fused_feat.unsqueeze(1)], dim=2)\n",
        "                output, hidden = self.decoder(combined, hidden)\n",
        "                output = self.fc(output.squeeze(1))\n",
        "\n",
        "                # Get predicted token\n",
        "                token = output.argmax(1)\n",
        "                generated[:, t] = token\n",
        "\n",
        "                # Update finished sequences\n",
        "                finished = finished | (token == vocab['<EOS>'])\n",
        "\n",
        "                # Stop if all sequences are finished\n",
        "                if finished.all():\n",
        "                    break\n",
        "\n",
        "                # Prepare next input\n",
        "                input_token = token.unsqueeze(1)\n",
        "                input_token[finished] = vocab['<EOS>']\n",
        "\n",
        "            return generated\n",
        "\n",
        "\n",
        "\n",
        "report_model = MultimodalReportGenerator(report_vocab_size=VOCAB_SIZE,hidden_dim=256 ).to(device)\n",
        "\n",
        "# Optimizer and loss\n",
        "report_optimizer = optim.Adam(report_model.parameters(), lr=1e-4)\n",
        "report_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "\n",
        "# Training function for report generator\n",
        "def train_report_generator(model, train_data, val_data, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Shuffle training data each epoch\n",
        "        import random\n",
        "        random.shuffle(train_data)\n",
        "\n",
        "        # Process in chunks to reduce memory\n",
        "        for i in range(0, len(train_data), 4):  # Reduced batch size from 8 to 4\n",
        "            if i + 4 > len(train_data):\n",
        "                continue\n",
        "\n",
        "            batch = train_data[i:i+4]\n",
        "\n",
        "            # Prepare inputs\n",
        "            global_views = torch.stack([item['global_view'] for item in batch]).to(device)\n",
        "            saliency_views = torch.stack([item['saliency_view'] for item in batch]).to(device)\n",
        "            regions = [\n",
        "                torch.stack([item[f'region{i+1}'] for item in batch]).to(device)\n",
        "                for i in range(3)\n",
        "            ]\n",
        "\n",
        "            # Tokenize reports\n",
        "            captions = torch.stack([encode_caption(item['full_report'], vocab) for item in batch]).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(global_views, saliency_views, regions, captions)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = report_criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                captions.view(-1)\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            report_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            report_optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = train_loss / num_batches if num_batches > 0 else 0\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model with reduced epochs\n",
        "train_report_generator(report_model, train_multiview, val_multiview, epochs=10)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:23:57.824659Z",
          "iopub.execute_input": "2025-07-13T15:23:57.824847Z",
          "iopub.status.idle": "2025-07-13T15:36:04.974553Z",
          "shell.execute_reply.started": "2025-07-13T15:23:57.824832Z",
          "shell.execute_reply": "2025-07-13T15:36:04.97385Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n31T1scloIyB",
        "outputId": "d6fa4f6d-3925-4752-9339-5f4c9603d1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 5.1812\n",
            "Epoch 2/10 | Train Loss: 4.2544\n",
            "Epoch 3/10 | Train Loss: 3.9288\n",
            "Epoch 4/10 | Train Loss: 3.7367\n",
            "Epoch 5/10 | Train Loss: 3.6101\n",
            "Epoch 6/10 | Train Loss: 3.4972\n",
            "Epoch 7/10 | Train Loss: 3.4060\n",
            "Epoch 8/10 | Train Loss: 3.3267\n",
            "Epoch 9/10 | Train Loss: 3.2583\n",
            "Epoch 10/10 | Train Loss: 3.2274\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "################################\n",
        "### 6. Evaluation & Metrics ###\n",
        "################################\n",
        "\n",
        "def calculate_bleu(generated, references):\n",
        "    \"\"\"Calculate BLEU scores\"\"\"\n",
        "    # Tokenize reports\n",
        "    refs = [[word_tokenize(ref.lower())] for ref in references]\n",
        "    gens = [word_tokenize(gen.lower()) for gen in generated]\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    smoothing = SmoothingFunction().method4\n",
        "    bleu1 = corpus_bleu(refs, gens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
        "    bleu2 = corpus_bleu(refs, gens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
        "    bleu3 = corpus_bleu(refs, gens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
        "    bleu4 = corpus_bleu(refs, gens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
        "\n",
        "    return {\n",
        "        \"BLEU-1\": bleu1,\n",
        "        \"BLEU-2\": bleu2,\n",
        "        \"BLEU-3\": bleu3,\n",
        "        \"BLEU-4\": bleu4\n",
        "    }\n",
        "\n",
        "def evaluate_model(model, test_data):\n",
        "    model.eval()\n",
        "    generated_reports = []\n",
        "    reference_reports = [item['full_report'] for item in test_data]\n",
        "\n",
        "    # Evaluate in smaller batches to save memory\n",
        "    for i in range(0, len(test_data), 4):  # Reduced batch size\n",
        "        if i + 4 > len(test_data):\n",
        "            continue\n",
        "\n",
        "        batch = test_data[i:i+4]\n",
        "\n",
        "        # Prepare inputs\n",
        "        global_views = torch.stack([item['global_view'] for item in batch]).to(device)\n",
        "        saliency_views = torch.stack([item['saliency_view'] for item in batch]).to(device)\n",
        "        regions = [\n",
        "            torch.stack([item[f'region{i+1}'] for item in batch]).to(device)\n",
        "            for i in range(3)\n",
        "        ]\n",
        "\n",
        "        # Generate reports\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(global_views, saliency_views, regions)\n",
        "\n",
        "        # Decode reports (simplified)\n",
        "        for output in outputs:\n",
        "            words = []\n",
        "            for token in output:\n",
        "                token = token.item()\n",
        "                if token == vocab['<EOS>']:\n",
        "                    break\n",
        "                if token > vocab['<UNK>']:  # Avoid special tokens\n",
        "                    words.append(inv_vocab.get(token, '<UNK>'))\n",
        "            generated_reports.append(\" \".join(words))\n",
        "\n",
        "    # Calculate metrics\n",
        "    bleu_scores = calculate_bleu(generated_reports, reference_reports)\n",
        "\n",
        "    # Print sample reports\n",
        "    print(\"\\nSample Report:\")\n",
        "    print(f\"Generated: {generated_reports[0]}\")\n",
        "    print(f\"Reference: {reference_reports[0]}\")\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "# Evaluate model\n",
        "test_results = evaluate_model(report_model, test_multiview)\n",
        "print(\"\\nTest Results:\")\n",
        "for metric, score in test_results.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(report_model.state_dict(), \"radiology_report_generator.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-13T15:36:04.976254Z",
          "iopub.execute_input": "2025-07-13T15:36:04.976446Z",
          "iopub.status.idle": "2025-07-13T15:36:06.998361Z",
          "shell.execute_reply.started": "2025-07-13T15:36:04.976431Z",
          "shell.execute_reply": "2025-07-13T15:36:06.997676Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IYSnGDloIyC",
        "outputId": "4edcd85f-f3e2-49e1-af16-bbf23c95a666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Report:\n",
            "Generated: findings : the heart is normal . size . the lungs are clear . no acute cardiopulmonary abnormality .\n",
            "Reference: FINDINGS:\n",
            "Lungs are clear. Heart size normal. The XXXX are unremarkable.\n",
            "\n",
            "IMPRESSION:\n",
            "No acute cardiopulmonary finding.\n",
            "\n",
            "Test Results:\n",
            "BLEU-1: 0.3775\n",
            "BLEU-2: 0.2746\n",
            "BLEU-3: 0.2020\n",
            "BLEU-4: 0.1489\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g336iWnsufTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}